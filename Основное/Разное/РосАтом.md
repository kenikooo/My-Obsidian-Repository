— Перейти в [[HUB]] —
> [!BUG] **Задание выполнено неполностью и не все настройки могут быть корректными**

0. [[#Базовая сетевая настройка]]
1. [[#Клиенты и сервера, сетевая настройка]]
2. [[#hostname, на все устройства]]
3. [[#DNS, настройка на CR SRV]]
4. [[#Полноценная настройка NFTables на CR-RTR и BR-RTR]]
	5.1 [[#NFTables, __временный__ NAT для теста]]
5. [[#StrongSwan]]
6. [[#Настройка GRE туннеля на CR-RTR и BR-RTR]]
7. [[#FRR на CR-RTR и BR-RTR]]
8. [[#Wireguard CR-RTR -> OUT-CLI]]
9. [[#isc-dhcp-server на br-rtr]]
10. [[#Централизованный сбор логов на BR-SRV2]]
11. [[#Docker на BR-SRV1]]
    12.1 [[#Squid Proxy - BR-SRV1]]
	12.2 [[#Python Приложения]]
12. [[#EasyRsa на CR-SRV]]
13. [[#ИТОГОВЫЙ ВИД docker-compose.yml]]
14. [[#Ansible на BR-SRV1]]
---
> [!INFO ] Список имён DNS
> | Имя                    | Тип    | Адрес                                    |
> | ---------------------- | ------ | ---------------------------------------- |
> | 'hostname'.stud.skills | A, PTR | Адрес машины (для всех ВМ, кроме WIN-AD) |
> | api.stud.skills        | CNAME  | BR-SRV2                                  |
> | logs.stud.skills       | CNAME  | BR-SRV2                                  |
> | proxy.stud.skills      | CNAME  | BR-SRV2                                  |
> | ald.stud.skills        | CNAME  | CR-SRV                                   |

---
> [!BUG] Используемые версии ОС
| Имя хоста               | Операционная система  |
| ----------------------- | --------------------- |
| *-CLI                   | Astra Linux 1.8.1 GUI |
| CR-DB, BR-SRV1, BR-SRV2 | Astra Linux 1.8.1 CLI |
| CR-RTR, BR-RTR, ISP     | Debian 12.5           |
| CR-SRV                  | Astra Linux 1.7.5 CLI |
| WIN-AD                  | Windows Server 2022   |

---
> [!WARNING] Таблица адресации
| Имя устройства | IP-Адрес      | Маска |
| :------------: | ------------- | ----- |
|      ISP       | 1.2.1.1       | 29    |
|                | 2.2.1.1       | 29    |
|                | 3.10.10.1     | 24    |
|     CR-RTR     | 2.2.1.2       | 29    |
|                | 192.168.3.1   | 24    |
|     BR-RTR     | 1.2.1.2       | 29    |
|                | 192.168.2.1   | 24    |
|     BR-CLI     | DHCP          | 24    |
|    BR-SRV1     | 192.168.2.100 | 24    |
|    BR-SRV2     | 192.168.2.200 | 24    |
|     CR-SRV     | 192.168.3.10  | 24    |
|     CR-DB      | 192.168.3.200 | 24    |
|     CR-CLI     | 192.168.3.100 | 24    |

---
#НачалоНастройки
## Базовая сетевая настройка:
CR-RTR:
```bash
auto enp1s0
iface enp1s0 inet static
    address 1.2.1.2
    netmask 255.255.255.248
    gateway 1.2.1.1
    
auto enp6s0
iface enp6s0 inet static
    address 192.168.3.1
	netmask 255.255.255.0
```
BR-RTR:
```bash
auto enp1s0
iface enp1s0 inet static
    address 2.2.1.2
    netmask 255.255.255.248
    gateway 2.2.1.1
    
auto enp6s0
iface enp6s0 inet static
    address 192.168.2.1
	netmask 255.255.255.0
```
На обоих роутерах:
```bash
echo 'net.ipv4.ip_forward=1' | sudo tee -a /etc/sysctl.conf
```
---
## Клиенты и сервера, сетевая настройка
CR-SRV
```bash
auto eth0
iface eth0 inet static
    address 192.168.3.10
    netmask 255.255.255.0
    gateway 192.168.3.1
    dns-nameservers 192.168.3.10
```
BR-SRV1
```bash
auto enp1s0
iface enp1s0 inet static
    address 192.168.2.100
    netmask 255.255.255.0
    gateway 192.168.2.1
    dns-nameservers 192.168.3.10
```
BR-SRV2
```bash
auto enp1s0
iface enp1s0 inet static
    address 192.168.2.200
    netmask 255.255.255.0
    gateway 192.168.2.1
    dns-nameservers 192.168.3.10
```
OUT-CLI
```bash
auto eth0
iface eth0 inet static
    address 3.10.10.10
    netmask 255.255.255.0
    gateway 3.10.10.1
    dns-nameservers 192.168.3.10
```

На всех устройствах:
```bash
systemctl restart networking
```
---
## hostname, на все устройства:
```bash
# Офис HQ
hostnamectl set-hostname cr-rtr.stud.skills; exec bash
hostnamectl set-hostname cr-srv.stud.skills; exec bash
hostnamectl set-hostname cr-db.stud.skills; exec bash
# OUT
hostnamectl set-hostname out-cli.stud.skills; exec bash
# Офис BR
hostnamectl set-hostname br-rtr.stud.skills; exec bash
hostnamectl set-hostname br-srv1.stud.skills; exec bash
hostnamectl set-hostname br-srv2.stud.skills; exec bash
hostnamectl set-hostname br-cli.stud.skills; exec bash
```
---
### DNS, настройка на CR SRV:
```bash
apt update && apt install bind9 bind9utils bind9-doc -y
```
### Настройка named.conf.options и named.conf.local
```bash
vim /etc/bind/named.conf.options

# Содержимое:
options {
    directory "/var/cache/bind";
    
    forwarders {
        10.150.1.1;
    };
    
    dnssec-validation auto;
    
    listen-on { any; };
    listen-on-v6 { any; };
    
    allow-query { any; };
    
    recursion yes;
    
    allow-update { none; };
    
    version "not available";
};
---
vim /etc/bind/named.conf.local
# Содержимое
zone "stud.skills" {
    type master;
    file "/etc/bind/db.stud.skills";
    allow-transfer { 192.168.2.100; }; // BR-SRV1
    allow-update { 192.168.2.0/24; };
};

zone "3.168.192.in-addr.arpa" {
    type master;
    file "/etc/bind/db.192.168.3";
    allow-transfer { 192.168.2.100; };
    allow-update { 192.168.2.0/24; };
};

zone "2.168.192.in-addr.arpa" {
    type master;
    file "/etc/bind/db.192.168.2";
    allow-transfer { 192.168.2.100; };
};
---
systemctl restart bind9
systemctl enable bind9
```

Настройка зон:
### Зона stud.skills
```bash
vim /etc/bind/db.stud.skills
# Содержимое
$TTL    604800
@       IN      SOA     cr-srv.stud.skills. admin.stud.skills. (
                              2024010101    ; Serial
                          604800     ; Refresh
                           86400     ; Retry
                        2419200     ; Expire
                          604800 )   ; Negative Cache TTL

; NS records
@       IN      NS      cr-srv.stud.skills.
@       IN      NS      br-srv1.stud.skills.

; A records
cr-srv      IN      A       192.168.3.10
cr-db       IN      A       192.168.3.200
cr-cli      IN      A       192.168.3.100
cr-rtr      IN      A       192.168.3.1

br-srv1     IN      A       192.168.2.100
br-srv2     IN      A       192.168.2.200
br-rtr      IN      A       192.168.2.1
br-cli      IN      A       192.168.2.50
; br-cli будет динамическим

out-cli     IN      A       3.10.10.10
isp         IN      A       2.2.1.1

; CNAME records
api         IN      CNAME   br-srv2
logs        IN      CNAME   br-srv2
proxy       IN      CNAME   br-srv2
ald         IN      CNAME   cr-srv
```
### Зона db.192.168.3:
```bash
vim /etc/bind/db.192.168.3
# Содержимое
$TTL    604800
@       IN      SOA     cr-srv.stud.skills. admin.stud.skills. (
                              2024010101    ; Serial
                          604800     ; Refresh
                           86400     ; Retry
                        2419200     ; Expire
                          604800 )   ; Negative Cache TTL

@       IN      NS      cr-srv.stud.skills.
@       IN      NS      br-srv1.stud.skills.

10      IN      PTR     cr-srv.stud.skills.
200     IN      PTR     cr-db.stud.skills.
100     IN      PTR     cr-cli.stud.skills.
1       IN      PTR     cr-rtr.stud.skills.
```
### Зона db.192.168.2:
```bash
vim /etc/bind/db.192.168.2
$TTL    604800
@       IN      SOA     cr-srv.stud.skills. admin.stud.skills. (
                              2024010101    ; Serial
                          604800     ; Refresh
                           86400     ; Retry
                        2419200     ; Expire
                          604800 )   ; Negative Cache TTL

@       IN      NS      cr-srv.stud.skills.
@       IN      NS      br-srv1.stud.skills.

100     IN      PTR     br-srv1.stud.skills.
200     IN      PTR     br-srv2.stud.skills.
1       IN      PTR     br-rtr.stud.skills.
50      IN      PTR     br-cli.stud.skills.
```
---
## Настройка SSH на всех устройствах
```bash
vim /etc/ssh/sshd_config
# 1 CR-RTR, BR-RTR
# Содержимое
Port 22
PermitRootLogin no
PasswordAuthentication yes
PubkeyAuthentication yes
X11Forwarding no
AllowUsers user administrator

# CR-SRV, BR-SRV1, BR-SRV2, CR-DB, CLI машины
# Содержимое
Port 22
PermitRootLogin no
PasswordAuthentication yes
PubkeyAuthentication yes
X11Forwarding no
PrintMotd yes

systemctl restart ssh
systemctl enable ssh
```
---
## NFTables, __временный__ NAT для теста:
```bash
nft add table inet nat
nft add chain inet nat postrouting { type nat hook postrouting priority 100\; }
nft add rule inet nat postrouting oifname "enp1s0" masquerade
# Сохранение правил
nft list ruleset | sudo tee /etc/nftables.conf
systemctl enable nftables
systemctl restart nftables
# Проверка
nft list ruleset
```
---
## Полноценная настройка NFTables на CR-RTR и BR-RTR:
CR-RTR:
```bash
vim /etc/nftables.conf

# Содержимое
#!/usr/sbin/nft -f

flush ruleset

table inet filter {
    chain input {
        type filter hook input priority 0; policy drop;
        
        iifname "lo" accept
        ct state {established, related} accept
        iifname "tunnel0" accept
        # Разрешаем SSH на маршрутизатор
        tcp dport 22 accept
        ip saddr { 2.2.1.0/29 1.2.1.0/29 } icmp type {  echo-request, echo-reply } accept
        # Разрешаем ICMP из внутренних сетей
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } icmp type { echo-request, echo-reply } accept
        # Запрещаем ICMP из внешних сетей
        icmp type { echo-request, echo-reply } drop
        ip protocol ospf accept
    }
    
    chain forward {
        type filter hook forward priority 0; policy drop;
        ct state {established, related} accept
        ip saddr 10.255.255.0/30 accept
        ip daddr 10.255.255.0/30 accept
        # Разрешаем трафик между сетями
        ip saddr 192.168.3.0/24 ip daddr 192.168.2.0/24 accept
        ip saddr 192.168.2.0/24 ip daddr 192.168.3.0/24 accept
        # Разрешаем интернет для внутренних сетей
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } oifname "enp1s0" accept
    }
    
    chain output {
        type filter hook output priority 0; policy accept;
    }
}
table ip nat {
    chain postrouting {
        type nat hook postrouting priority 100; policy accept;
        oifname "enp1s0" masquerade
    }
    chain prerouting {
        type nat hook prerouting priority -100; policy accept;
        iifname "enp1s0" tcp dport 2222 dnat to 192.168.3.10:22
    }
}
```

BR-RTR
```bash
#!/usr/sbin/nft -f

flush ruleset

table inet filter {
    chain input {
        type filter hook input priority 0; policy drop;
        
        iifname "lo" accept
        ct state {established, related} accept
        iifname "tunnel0" accept
        tcp dport 22 accept
        ip saddr { 2.2.1.0/29 1.2.1.0/29 } icmp type {  echo-request, echo-reply } accept
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } icmp type { echo-request, echo-reply } accept
        icmp type { echo-request, echo-reply } drop

        ip protocol ospf accept
    }
    
    chain forward {
        type filter hook forward priority 0; policy drop;
        ct state {established, related} accept
        ip saddr { 2.2.1.2 1.2.1.2 } ip daddr { 2.2.1.2 1.2.1.2 } ip protocol 47 accept
        ip saddr 10.255.255.0/30 accept
        ip daddr 10.255.255.0/30 accept
        ip saddr 192.168.3.0/24 ip daddr 192.168.2.0/24 accept
        ip saddr 192.168.2.0/24 ip daddr 192.168.3.0/24 accept
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } oifname "enp1s0" accept
    }
    
    chain output {
        type filter hook output priority 0; policy accept;
    }
}

table ip nat {
    chain postrouting {
        type nat hook postrouting priority 100; policy accept;
        oifname "enp1s0" masquerade
    }
}
```
Применяем правила:
```bash
nft -c -f /etc/nftables.conf # проверка синтаксиса
nft -f /etc/nftables.conf # применяем, если все в норме
systemctl enable nftables
systemctl restart nftables
# Проверяем
nft list ruleset
```
---
## StrongSwan
```bash
apt install strongswan strongswan-pki -y
```
Переходим в конфиг (CR-RTR):
```bash
vim /etc/ipsec.conf
# Содержимое
config setup
	charondebug="ike 1, knl 1, cfg 0"

conn %default
	ikelifetime=60m
	keylife=20m
	rekeymargin=3m
	keyingtries=1
	keyexchange=ikev2
	authby=secret

conn gre-protect
	left=10.255.255.2
	right=10.255.255.1
	leftprotoport=gre
	rightprotoport=gre
	type=tunnel
	auto=start
	authby=secret
	keyexchange=ikev2
```
Переходим в конфиг (BR-RTR):
```bash
vim /etc/ipsec.conf
# Содержимое
config setup
	charondebug="ike 1, knl 1, cfg 0"

conn %default
	ikelifetime=60m
	keylife=20m
	rekeymargin=3m
	keyingtries=1
	keyexchange=ikev2
	authby=secret

conn gre-protect
	left=10.255.255.1
	right=10.255.255.2
	leftprotoport=gre
	rightprotoport=gre
	type=tunnel
	auto=start
	authby=secret
	keyexchange=ikev2
```
На обоих устройствах, меняем адреса на туннельные и ставим свой пароль:
```bash
vim /etc/ipsec.secrets
# Содержимое
10.255.255.1 10.255.255.2 : PSK "P@ssw0rd"
```
Перезагружаем службу:
```bash
ipsec reload
ipsec restart
systemctl restart strongswan-starter.service
```

---
## Настройка GRE туннеля на CR-RTR и BR-RTR:
```bash
CR-RTR
vim /etc/network/interfaces
# Содержимое
auto tunnel0
iface tunnel0 inet tunnel
    address 10.255.255.1
    netmask 255.255.255.252
    mode gre
    local 1.2.1.2
    endpoint 2.2.1.2
    ttl 255

BR-RTR
# Содержимое
auto tunnel0
iface tunnel0 inet tunnel
    address 10.255.255.2
    netmask 255.255.255.252
    mode gre
    local 2.2.1.2
    endpoint 1.2.1.2
    ttl 255
    
/etc/modules
> ip_gre
```
---
## FRR на CR-RTR и BR-RTR
```bash
apt install frr -y
vim /etc/frr/daemons
> ospfd = yes
# CR-RTR
vtysh
> conf t
> router ospf
> network 192.168.3.0/24 area 0
> network 10.255.255.0/30 area 0
> passive interface default
interface tunnel 0
> no ip ospf passive
# BR-RTR
vtysh
> conf t
> router ospf
> network 192.168.2.0/24 area 0
> network 10.255.255.0/30 area 0
> passive interface default
interface tunnel 0
> no ip ospf passive
# Общие команды проверки
> show ip ospf neighbor
> show ip ospf route
> show ip route ospf
```
---
## Wireguard CR-RTR -> OUT-CLI

CR-RTR и OUT-CLI:
```bash
mkdir -p /etc/wireguard
cd /etc/wireguard
wg genkey | sudo tee private.key | sudo wg pubkey | sudo tee public.key
chmod 600 private.key
# Смотрим публичный ключ (запомним для OUT-CLI)
cat public.key
```

Конфиг CR-RTR:
```bash
vim /etc/wireguard/wg0.conf
# Содержимое
[Interface]
PrivateKey = <содержимое_private.key_CR-RTR>
Address = 10.10.10.1/24
ListenPort = 51820
[Peer]
PublicKey = <содержимое_public.key_OUT-CLI>
AllowedIPs = 10.10.10.2/32
```
Конфиг OUT-CLI:
```bash
vim /etc/wireguard/wg0.conf
# Содержимое
[Interface]
PrivateKey = <содержимое_private.key_CR-RTR>
Address = 10.10.10.2/30

[Peer]
PublicKey = <содержимое_public.key_OUT-CLI>
Endpint = 2.2.1.2:51820
AllowedIPs = 0.0.0.0/0
PersistentKeepalive = 25
```
Для обоих устройств:
```bash
systemctl enable wg-quick@wg0
systemctl start wg-quick@wg0
systemctl restart wg-quick@wg0
```
---
## isc-dhcp-server на br-rtr
В самом начале устанавливаем пакет:
```bash
apt update && apt install isc-dhcp-service
```
Переходим в конфигурационный файл:
```bash
/etc/dhcp/dhcpd.conf
# Содержимое:
option domain-name "stud.skills";
option domain-name-servers 192.168.2.100;

default-lease-time 600;
max-lease-time 7200;
authoritative;

subnet 192.168.2.0 netmask 255.255.255.0 {
	range 192.168.2.50 192.168.2.99;
	option routers 192.168.2.1;
	option subnet-mask 255.255.255.0;
	option domain-name-servers 192.168.2.100;
	ddns-domainname "stud.skills.";
	ddns-rev-domainname "in-addr.arpa.";
	update-static-leases on;
	update-conflict-detection off;
}

# Включаем и перезагружаем службу:
systemctl enable isc-dhcp-service
systemctl restart isc-dhcp-service
# После чего, проверяем статус:
systemctl status isc-dhcp-service
```
## Централизованный сбор логов на BR-SRV2
```bash
apt update && apt install rsyslog -y
```
Переходим к ротации:
```bash
vim /etc/logrotate.d/remote-logs
# Содержимое
/opt/logs/*.log {
	missingok
	rotate 5
	compress
	delaycompress
	size 10M
	copytruncate
	create 644 syslog syslog
	postrotate
		/usr/lib/rsyslog/rsyslog-rotate
	endscript
}
```
Идем в конфиг файл:
```bash
# Раскоментируем или добавим:
module(load="imudp")
input(type="imudp" port="514")

module(load="imtcp")
input(type="imtcp" port="514")
```
Создаем файлик:
```bash
vim /etc/rsyslog.d/50-default.conf
# Содержимое
$template RemoteLogs,"/opt/logs/%HOSTNAME%.log"
*.* ?RemoteLogs

:fronthost-ip, isequal, "192.168.3.1" -
:fronthost-ip, isequal, "192.168.2.1" -
```
Создание пользователя, и директории для логов:
```bash
# Создаем группу и пользователя
groupadd -r syslog
useradd -r -s /bin/false -g syslog -c "System Logging User" syslog
# Создаем директорию и присваиваем ей права
mkdir -p /opt/logs
chown syslog:syslog /opt/logs
```
На CR-RTR и BR-RTR, устанавливаем rsyslog и создаем файл:
```bash
vim /etc/rsyslog.d/50-forward.conf
# Содержимое везде одинаковое
*.* @192.168.2.200:514
```
Запускаем службу (везде) и перезагружаем:
```bash
systemctl enable rsyslog
systemctl restart rsyslog
```
Проверяем:
```bash
# На BR-SRV2 смотрим файлы логов:
ls -la /opt/logs
# Должны появится br-rtr.log и cr-rtr.log

# Идем к примеру на CR-RTR и пишем следующее:
logger "Test log"

# Возвращаемся на BR-SRV2, проверяем:
tail -f /opt/logs/cr-rtr.log
> Вывод: <Пользователь>: Test log
```

Настройка Web-доступа к логам на BR-SRV2:
```bash
apt install nginx apache2-utils -y 
# После установки, сразу же создаем пользователя
htpasswd -bc /etc/nginx/.htpasswd logadmin logpass
```
Создаем приватный ключ для nginx:
```bash
# На BR-SRV2 создаем самоподписанный сертификат
sudo mkdir -p /etc/ssl/private
sudo mkdir -p /etc/ssl/certs
# Создаем приватный ключ
sudo openssl genrsa -out /etc/ssl/private/nginx-selfsigned.key 2048
# Создаем сертификат
sudo openssl req -new -x509 -key /etc/ssl/private/nginx-selfsigned.key \
    -out /etc/ssl/certs/nginx-selfsigned.crt \
    -days 365 -subj "/CN=logs.stud.skills"
```
Настраиваем права доступа:
```bash
chmod 600 /etc/ssl/private/nginx-selfsigned.key
chmod 644 /etc/ssl/certs/nginx-selfsigned.crt
```
Переходи в конфиг файл:
```bash
/etc/nginx/sites-available/logs
# Содержимое
server {
    listen 80;
    server_name _;
    return 404;
}
server {
	listen 80;
	server_name logs.stud.skills;
    return 301 https://$host$request_uri;
}
server {
    listen 443 ssl;
    server_name logs.stud.skills;
    ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt;
    ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key;
    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/.htpasswd;
    location / {
        root /opt/logs;
        autoindex on;
    }
}
```
Включаем сайт:
```bash
ln -s /etc/nginx/sites-available/logs /etc/nginx/sites-enabled/
nginx -t
systemctl restart nginx
```
Проверяем с любого ПК:
```bash
https://logs.stud.skills
# Логин: logadmin
# Пароль: logpass
```

## Docker на BR-SRV1
```bash
apt update && apt install docker.io docker-compose-v2 -y
# Добавляем пользователя в группу docker
usermod -aG docker $USER
newgrp docker
# Запускаем службу
sudo systemctl enable docker
sudo systemctl start docker
# Проверяем
docker --version
docker-compose --version
```
Создаем рабочую директорию:
```bash
mkdir -p /opt/compose
cd /opt/compose
vim docker-compose.yml
# Содержимое
version: '3.8'
services:
  postgres:
    image: postgres:13
    container_name: postgres-db
    environment:
      POSTGRES_DB: university
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: P@ssw0rd
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database.sql:/docker-entrypoint-initdb.d/database.sql
    restart: unless-stopped
    
volumes:
  postgres_data:
```
Создаем дамп базы данных
```sql
vim database.sql
# Содержимое
-- groups
CREATE SCHEMA IF NOT EXISTS learning;
CREATE SCHEMA IF NOT EXISTS accounting;
-- tables
CREATE TABLE public.students (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    group_name VARCHAR(50)
);
CREATE TABLE learning.subjects (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    credits INTEGER
);
CREATE TABLE learning.marks (
    id SERIAL PRIMARY KEY,
    student_id INTEGER REFERENCES public.students(id),
    subject_id INTEGER REFERENCES learning.subjects(id),
    mark INTEGER,
    date DATE
);
CREATE TABLE accounting.attendance (
    id SERIAL PRIMARY KEY,
    student_id INTEGER REFERENCES public.students(id),
    date DATE,
    present BOOLEAN
);
CREATE TABLE accounting.reasons (
    id SERIAL PRIMARY KEY,
    attendance_id INTEGER REFERENCES accounting.attendance(id),
    reason TEXT
);
-- roles
CREATE ROLE student;
CREATE ROLE teacher;  
CREATE ROLE administration;
-- user
CREATE USER sidorov WITH PASSWORD 'P@ssw0rd';
CREATE USER petrov WITH PASSWORD 'P@ssw0rd';
CREATE USER smirnov WITH PASSWORD 'P@ssw0rd';
--grants
GRANT student TO sidorov;
GRANT teacher TO petrov;
GRANT administration TO smirnov;
GRANT USAGE ON SCHEMA learning TO student;
GRANT SELECT ON ALL TABLES IN SCHEMA learning TO student;
GRANT USAGE ON SCHEMA learning, accounting TO teacher;
GRANT SELECT, INSERT, UPDATE ON learning.marks TO teacher;
GRANT SELECT, INSERT, UPDATE ON accounting.reasons TO teacher;
GRANT SELECT, INSERT, UPDATE ON accounting.attendance TO teacher;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public, learning, accounting TO administration;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public, learning, accounting TO administration;
-- test data
INSERT INTO public.students (name, group_name) VALUES 
('Алексей Иванов', 'Группа 101'),
('Егор Наградов', 'Группа 102');
INSERT INTO learning.subjects (name, credits) VALUES
('Математика', 5),
('Физика', 4);
```
Запуск docker compose:
```bash
cd /opt/compose
docker compose up -d
# Проверяем
docker ps
docker logs postgres-db
```
Проверяем базы данных:
```bash
# установка клиента PostgreSQL
apt install postgresql-client -y

# подключение к базе
psql -h localhost -U sidorov -d university -W
# пароль: P@ssw0rd

# Проверяем доступ
\dt learning.*
\dt accounting.*
\dt public.*
```
Возможные исправления ошибок:
```bash
docker ps
docker logs postgres-db
psql -h localhost -U postgres -d university -W
# пароль: P@ssw0rd

# проверяем что пользователи создались
\du
```
Проверка Postgres:
```bash
psql -h localhost -U postgres -d university -W
# Пароль: P@ssw0rd

# проверяем пользователей
\du

# подключаемся как smirnov
psql -h localhost -U smirnov -d university -W
# Пароль: P@ssw0rd

\dt *.*
```
## Squid Proxy - BR-SRV1
Переходим в ранее созданную /opt/compose:
```bash
cd /opt/compose
vim docker-compose.yml
# Дополняем файл:
version: '3.8'

services:
  postgres:
    image: postgres:13
    container_name: postgres-db
    environment:
      POSTGRES_DB: university
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: P@ssw0rd
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database.sql:/docker-entrypoint-initdb.d/database.sql
    restart: unless-stopped
	networks:
	  - backend
# Добавлено следующее (описано ниже):
  squid:
    image: sameersbn/squid:3.5.27-2
    container_name: squid-proxy
    ports:
      - "8081:3128"
    environment:
      - SQUID_PROXY_PORT=3128
    volumes:
      - ./squid.conf:/etc/squid/squid.conf
      - squid_cache:/var/spool/squid
    restart: unless-stopped
	networks:
	  - backend
	    
volumes:
  postgres_data:
  # также добавлен volume:
  squid_cache:

networks:
	backend:
		driver: bridge
```
Создаем конфиг для squid proxy:
```bash
vim squid.conf
# Содержимое
http_port 3128
visible_hostname proxy.stud.skills
cache_dir ufs /var/spool/squid 100 16 256
maximum_object_size 256 MB
acl local_net src 192.168.2.0/24
acl local_net src 192.168.3.0/24
acl site1 dstdomain site1.tech.skills
acl site2 dstdomain site2.tech.skills
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic realm Squid proxy
acl authenticated proxy_auth REQUIRED
http_access allow local_net site2
http_access allow authenticated site1
http_access deny site2
http_access allow local_net
http_access deny all
```
Создаем файл с пользователями:
```bash
# Создаем пользователя mikhail
sh -c "echo 'mikhail:proxypass' > /opt/compose/squid.passwd"
# Хешируем пароль для Squid
apt update && apt install apache2-utils -y
htpasswd -b -c /opt/compose/squid.passwd mikhail proxypass
```
Включаем и проверяем:
```bash
# Включение
docker-compose up -d squid
# Проверка
docker ps
curl -I http://proxy.stud.skills:8081
```
Проблема с Squid:
```bash
# Проверяем контейнер Squid
docker ps | grep squid

# Смотрим логи Squid
docker logs squid-proxy

# Частая проблема - нет файла passwd
docker exec -it squid-proxy ls -la /etc/squid/
```
Проверка Squid:
```bash
# Тестируем proxy
curl -v --proxy http://localhost:8081 http://google.com

# Тестируем аутентификацию
curl -v --proxy http://mikhail:proxypass@localhost:8081 http://site1.tech.skills
```

## Python Приложения
```bash
# Добавляем сервисы в докер
python-app1:
	image: python:3.12-alpine
	container_name: python-app-1
	working-dir: /app
	volumes:
	  - "5001:5000"
	environment:
	  - INSTANCE_ID=app1
	command: sh -c "pip install -r requirments>txt && python app.py"
	restart: unless stopped
	networks:
	  - backend

python-app2:
	image: python:3.12-alpine
	container_name: python-app-2
	working-dir: /app
	volumes:
	  - "5002:5000"
	environment:
	  - INSTANCE_ID=app1
	command: sh -c "pip install -r requirments>txt && python app.py"
	restart: unless stopped
	networks:
	  - backend
	    
  haproxy-balancer:
    image: haproxy:2.8
    container_name: haproxy-balancer
    ports:
      - "8443:8443"
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
      - ./ssl/cert.pem:/usr/local/etc/haproxy/cert.pem
    depends_on:
      - python-app-1
      - python-app-2
    restart: unless-stopped
    networks:
      - backend
    
```
Создаем директорию:
```bash
mkdir app
vim app/app.py
# Содержимое:
from flask import Flask, jsonify
import os

app = Flask (__name__)

@app.route('/')
def hello():
	instance_id = os.getenv('INSTANCE_ID', 'unknown')
	return jsonify(
		'message': f'Hello from {instance_id}',
		'instance': instance_id

@app.route('/health')
def health():
	return jsonify({'status': 'healthy'})

if __name__ == '__main__';
	app.run(host='0.0.0.0', port=5000, debug=False)
```
Создаем файл requirments.txt:
```bash
app/requirments.txt
# Содерижое
Flask==3.0.0
```
Настраиваем HAProxy:
```bash
vim /opt/compose/haproxy.cfg
# Содержимое
global
    daemon
    maxconn 256
    log 127.0.0.1 local0

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    option forwardfor
    option httplog

frontend http_front
    bind *:8443 ssl crt /usr/local/etc/haproxy/cert.pem
    http-request set-header X-Forwarded-Proto https if { ssl_fc }
    default_backend http_back

backend http_back
    balance roundrobin
    server app1 python-app-1:5000 check
    server app2 python-app-2:5000 check

listen stats
    bind *:1936
    stats enable
    stats uri /
    stats hide-version
```
Создаем SSL сертификат для HAProxy:
```bash
mkdir -p ssl
cd ssl
sudo openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 \
    -keyout cert.key -out cert.crt \
    -subj "/CN=api.stud.skills"

# Объединяем в один файл для HAProxy
cat cert.crt cert.key > cert.pem
```
## EasyRsa на CR-SRV
```bash
apt install easy-rsa -y
# Создаем директорию
mkdir -p /etc/ca/easy-rsa
cd /etc/ca/easy-rsa
cp -r /usr/share/easy-rsa/* .
chmod 600 /etc/ssl/private/*.key
chown root:root /etc/ssl/private/*.key
```
Переходим к настройке VARS:
```bash
vim vars
# Содержимое
set_var EASYRSA_REQ_COUNTRY    "RU"
set_var EASYRSA_REQ_PROVINCE   "Moscow"
set_var EASYRSA_REQ_CITY       "Moscow"
set_var EASYRSA_REQ_ORG        "STUD-SKILLS"
set_var EASYRSA_REQ_EMAIL      "admin@stud.skills"
set_var EASYRSA_REQ_OU         "IT"
set_var EASYRSA_ALGO           "ec"
set_var EASYRSA_DIGEST         "sha512"
```

```bash
./easyrsa init-pki
# Создаем корневой CA
./easyrsa build-ca nopass
# Common Name: STUD-CA

# Создаем сертификаты для сервера
./easyrsa build-server-full br-srv2.stud.skills nopass
./easyrsa build-server-full api.stud.skills nopass
./easyrsa build-server-full logs.stud.skills nopass
./easyrsa build-server-full proxy.stud.skills nopass
./easyrsa build-server-full cr-srv.stud.skills nopass
./easyrsa build-client-full out-cli.stud.skills nopass
# Экспортируем сертификаты
cp pki/ca.crt /etc/ca/cacert.pem
cp pki/issued/br-srv2.stud.skills.crt /etc/ca/br-srv2.crt
cp pki/private/br-srv2.stud.skills.key /etc/ca/br-srv2.key
```
Для BR-SRV2 (логи):
```bash
mkdir -p /etc/ssl/private /etc/ssl/certs
cp pki/private/logs.stud.skills.key /etc/ssl/private/
cp pki/issued/logs.stud.skills.crt /etc/ssl/certs/
cp pki/ca.crt /etc/ssl/certs/stud-ca.crt
```
BR-SRV2, nginx:
```bash
vim /etc/nginx/sites-available/logs
# Обновляем секцию SSL
server {
    listen 443 ssl;
    server_name logs.stud.skills;
    
    ssl_certificate /etc/ssl/certs/logs.stud.skills.crt; # вносим изменения
    ssl_certificate_key /etc/ssl/private/logs.stud.skills.key; # вносим изменения
    ssl_trusted_certificate /etc/ssl/certs/stud-ca.crt; # добавляем
    
    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/.htpasswd;
    
    location / {
        root /opt/logs;
        autoindex on;
    }
}
```
Распространим корневой сертификат на все хосты (с CR-SRV):
```bash
scp /etc/ca/easy-rsa/pki/ca.crt user@br-srv1:/tmp/stud-ca.crt
scp /etc/ca/easy-rsa/pki/ca.crt user@br-srv2:/tmp/stud-ca.crt
scp /etc/ca/easy-rsa/pki/ca.crt user@br-cli:/tmp/stud-ca.crt
scp /etc/ca/easy-rsa/pki/ca.crt user@out-cli:/tmp/stud-ca.crt
# На каждой машине после копирования:
update-ca-certificates
# Проверяем работоспособность:
curl -v https://logs.stud.skills
```

### ИТОГОВЫЙ ВИД docker-compose.yml
```yml
version: '3.8'

services:
  postgres:
    image: postgres:13
    container_name: postgres-db
    environment:
      POSTGRES_DB: university
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: P@ssw0rd
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database.sql:/docker-entrypoint-initdb.d/database.sql
    restart: unless-stopped
    networks:
      - backend

  squid:
    image: sameersbn/squid:3.5.27-2
    container_name: squid-proxy
    ports:
      - "8081:3128"
    environment:
      - SQUID_PROXY_PORT=3128
    volumes:
      - ./squid.conf:/etc/squid/squid.conf
      - squid_cache:/var/spool/squid
    restart: unless-stopped
    networks:
      - backend

  python-app-1:
    image: python:3.12-alpine
    container_name: python-app-1
    working_dir: /app
    volumes:
      - ./app:/app
    ports:
      - "5001:5000"
    environment:
      - INSTANCE_ID=app1
    command: sh -c "pip install flask && python app.py"
    restart: unless-stopped
    networks:
      - backend

  python-app-2:
    image: python:3.12-alpine
    container_name: python-app-2
    working_dir: /app
    volumes:
      - ./app:/app
    ports:
      - "5002:5000"
    environment:
      - INSTANCE_ID=app2
    command: sh -c "pip install flask && python app.py"
    restart: unless-stopped
    networks:
      - backend

  haproxy-balancer:
    image: haproxy:2.8
    container_name: haproxy-balancer
    ports:
      - "8443:8443"
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
      - ./ssl/cert.pem:/usr/local/etc/haproxy/cert.pem
    depends_on:
      - python-app-1
      - python-app-2
    restart: unless-stopped
    networks:
      - backend

volumes:
  postgres_data:
  squid_cache:

networks:
  backend:
    driver: bridge
```
## Ansible на BR-SRV1
```bash
apt update && apt install ansible -y
mkdir -p /etc/ansible
cd /etc/ansible
```
Создаем inventory файл:
```bash
vim inventory.yml
# Содержимое:
all:
  hosts:
    br-cli.stud.skills:
      ansible_user: user
      ansible_ssh_pass: P@ssw0rd
      ansible_connection: ssh
    cr-cli.stud.skills:
      ansible_user: user  
      ansible_ssh_pass: P@ssw0rd
      ansible_connection: ssh
```
Создаем плейбук для сбора информации:
```bash
vim get_info.yml
# Содержимое:
---
- name: Collect client information
  hosts: all
  gather_facts: yes
  
  tasks:
    - name: Create reports directory
      file:
        path: /etc/ansible/reports
        state: directory
        mode: 0755
        
    - name: Collect system information
      setup:
        gather_subset:
          - network
          - hardware
          
    - name: Generate report
      copy:
        content: |
          hostname: {{ ansible_hostname }}
          ip_address: {{ ansible_default_ipv4.address }}
          mac_address: {{ ansible_default_ipv4.macaddress }}
          os: {{ ansible_distribution }} {{ ansible_distribution_version }}
        dest: "/etc/ansible/reports/{{ inventory_hostname }}.yml"
        mode: 0644
```
Запуск Ansible:
```bash
# Тест подключения
ansible -i inventory.yml all -m ping

# Запускаем плейбук
ansible-playbook -i inventory.yml get_info.yml

# Проверяем логи
ls -la /etc/ansible/reports/
cat /etc/ansible/reports/br-cli.stud.skills.yml
```
